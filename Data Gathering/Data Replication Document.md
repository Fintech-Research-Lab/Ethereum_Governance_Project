This document replicates the Ethereum project data creation document. This data is designed to empirically test how Ethereum makes decision about finalizing Ethereum Improvement Proposals (EIPs). This project will collect data on EIPs, their status and their authors. This will be supplemented by information about the authors like their twitter and github following, their current and past work history, job titles, and their author's cross-collaboration. We will also identify whether authors belong to Ethereum Client or not. In addition to author information, we will also look at the information about the content of EIPs, we will look at the sentiments on different EIPs by Ethereum community on Magician and we will catagorize discussion topics of Ethereum's development calls where these EIPs are discussed. 
# Data Collection Process
The data collection process begins with scraping of data from the website ethereum.org. This website contains information of all EIPs, their status and their authors. This data is collected using a web-scrapping code as of **06/15/2023**. The following steps were taken:
* Use code *?????????* to create a file which contains 639 EIPs. This file contains EIP Number, Authors, Status, and Title.
* Modify data to split Authors in upto 11 authors if EIPs are co-authored and provided a unique author_id. The result is the file called *AllEIPs.csv* This process is not replicable as some of it was done via code but later modified. However, once the file *allEIPS.csv* was created, we verified the author names and ids to ensure that from now on, author_ids are used. As a reference, we have stored that information in a file called *author.dta*
* We collect Github Data using a code *??????.py* which is stored in *Github_Data.csv*.
## Manual Collection
We manually collect the following data
* Twitter Data: We manually collected information on the authors' Twitter following and Twitter followers, if available. This data is in *Twitter_Data.csv*.
* LinkedIn Data: We also manually collected data from LinkedIn, capturing details of up to four current companies where the authors are presently employed, along with their job titles. Additionally, we gathered information on up to the last 10 companies where they had previously worked, including their past job titles. This is in *LinkedIn_Data.csv*
* Past Job Titles: For the most recent four companies where the authors worked, we collected the past job titles as follows:
  - For the most recent past company, we obtained three previous job titles.
  - For the two companies before the most recent past company, we collected two previous job titles for each.
  - Lastly, for the fourth oldest company where the author worked, we recorded one previous job title.
## Creating Cross-Sectional Ethereum Data by Merging 
We use a stata code called *data merging code.do* to create the cross-sectional data organized by EIP_Number. The following steps describe the process:
* The cross-sectional data is generated by first importing *AllEIPs.csv*. This file is converted into a stata file.
* Twitter, github, and LinkedIn data is merged to this stata file based on unique author_ids. This create the cross-sectional ethereum data based on EIPs. This data is stored in *Ethereum_Cross-sectional_Data.dta*. 
## Generating Commit Data
Apart from the cross-sectional data which is organized by *eip_number* we have also gathered time series data of github commitments from github. There are two sets of commit data
*  **EIP Commit** are commitments to github for each EIP. This data is organized by date and EIP.
    - We collect all commitments made by any contributor to the EIP, whether they are an author or a contributor who may not be an author. This collection is done through a python code *???????.py* the output of this code is a file *updated_commits.xlsx*
    - We use a stata code *eip commit data  creation code.do* which takes this data and merge it with *author.dta* to flag whether the github username that contributes to EIP is part of the author list or not. If it is a part of the author list we create a flag *eip_author = 1/0* indicating whether commitor is an eip author or not.
    - We aggregate the total number of commits to each EIP and merge it with the cross-sectional data in the *data merging code.do* and call it *total_commits*
    - We also aggregate the unique number of participants that are commiting to the eip github for each eip. We call this variable *contributors* and merge this with the cross-sectional data in the *data merging code.do*
*  ** Client Commit** are commitments to the *Client Repositories* These commitments may be for anything even things unrelated to EIPs. Later we match the github usernames of contributors of this data to our author list and add this data as an entry in the cross-sectional data.
The steps involve gathering all commit data from github. As a first step, we gather information from the github *EIP Respoistory* then we collect data from github's client repositories. We use top 4 clients *geth*,*nethermind*,*besu*, and *erigon*.  The following steps were taken to collect this data:
* Use code *?????????* to web scrape data from github's EIP repository to find commits by date, author (github username), commit hash code, and commit title. This data is available in file called *updated_commit.xlsx*.
* Matched this commit data to a list of author_ids that contains github_usernames so that we can identify whether the commit contributor was an EIP Author. This code is produced in *commit data creation code.do*.
  - First import the file *updated_commit.xlsx*  This file is a time series file as it contains dates of commitments. We store it as a stata file *ethereum_commit.dta*.It is a panel data which is organized by EIPs and date.
  - We count the total number of commits for each EIP and merge this information with *Ethereum_Cross-sectional_data.dta*
  - The data is saved as *ethereum_commit.dta*
## Create Betweenness Centrality Measure
we use python code *??????.py* to create a betweenness centrality measure amongs all co-authors of EIPs. The code takes file *allEIPs.csv* and create a betweenness centrality measure for each EIP. We merge the betweenness_centrality measure in the Ethereum_Cross-sectional_Date.dta using eip_number.
