This document replicates the Ethereum project data creation document. This data is designed to empirically test how Ethereum makes decision about finalizing Ethereum Improvement Proposals (EIPs). This project will collect data on EIPs, their status and their authors. This will be supplemented by information about the authors like their twitter and github following, their current and past work history, job titles, and their author's cross-collaboration. We will also identify whether authors belong to Ethereum Client or not. In addition to author information, we will also look at the information about the content of EIPs, we will look at the sentiments on different EIPs by Ethereum community on Magician and we will catagorize discussion topics of Ethereum's development calls where these EIPs are discussed. We also collect information on EIP commits on Github as well as commits made by Ethereum Clients to see if those factors like authorship by clients or intensity of Github commits influences EIP's finalization.
# Data Collection Process
The data collection process begins with scraping of data from the website ethereum.org. This website contains information of all EIPs, their status and their authors. This data is collected using a web-scrapping code as of **06/21/2023**. The following steps were taken:
* Use code *EIP list generating code.py* to create a file which contains 639 EIPs. This file contains EIP Number, Authors, Status, and Title. Note that running this code will produce the current state of data. Therefore, to replicate our data, one should use the file *allEIPsandAuthorsv2.csv*
* Once the raw data is downloaded, we apply the python code *pythoncode to prepare data merging*. This code matches author names to unique author list which we compiled separately. The reason is that we want to assign a numeric *author_id* which can identify each author. Our subsequent data collection process will use this author_id. Several repeat author formats are changed in this code so that it matches to the list with author_id. As a reference, we have stored author id information in a file called *unique_author_names_with_id.csv*
* We collect Github Data using a code *Github Follower Extract.py* which is stored in *Github_Data.csv*.
* We collect start date of all eips using the code *startdate_scraping.py* 
## Manual Collection
We manually collect the following data
* Twitter Data: We manually collected information on the authors' Twitter following and Twitter followers, if available. This data is in *Twitter_Data.csv*.
* LinkedIn Data: We also manually collected data from LinkedIn, capturing details of up to four current companies where the authors are presently employed, along with their job titles. Additionally, we gathered information on up to the last 10 companies where they had previously worked, including their past job titles. This is in *LinkedIn_Data.csv*
* Past Job Titles: For the most recent four companies where the authors worked, we collected the past job titles as follows:
  - For the most recent past company, we obtained three previous job titles.
  - For the two companies before the most recent past company, we collected two previous job titles for each.
  - Lastly, for the fourth oldest company where the author worked, we recorded one previous job title.
*  end date of all EIPs that have reached final stage
## Creating Cross-Sectional Ethereum Data by Merging 
We use a stata code called *data merging code.do* to create the cross-sectional data organized by EIP_Number. The following steps describe the process:
* The cross-sectional data is generated by first importing *Ethereum_Crossectional_Data.csv*. This is the output of running python code *pythoncode to prepare data merging*
* Twitter, github, start and end dates for final eips, and LinkedIn data is merged to this stata file based on unique author_ids. This create the cross-sectional ethereum data based on EIPs. This data is stored in *Ethereum_Cross-sectional_Data.dta*. 
## Generating Commit Data
Apart from the cross-sectional data which is organized by *eip_number* we have also gathered time series data of github commitments from github. There are two sets of commit data
*  **EIP Commit** are commitments to github's EIP repository. This data is organized by date and EIP.
    - We collect all commitments made by any contributor to the EIP, whether they are an author or a contributor who may not be an author. This collection is done through a python code *???????.py* the output of this code is a file *updated_commits.xlsx*
    - We use a stata code *eip commit data creation code.do* which takes this data and merge it with *author.dta*. If the merged value matches author.dta we flag the github_username *eip_author = 0/1*. If the github_usernames does not exists in the author.dta (_merge == 1) then we consider it as whether the github username that contributes to EIP is part of the author list or not. If it is a part of the author list we create a flag *eip_author = 1/0* indicating whether commitor is an eip author or not. We then aggregate this data to create cross-sectional equivalent based on eip_number to get the following three values:
    - Total number of commits to each EIP and merge it with the cross-sectional data in the *data merging code.do* and call it *total_commits*
    - Author Commits which is number of commits made by EIP Authors and merge it with the cross-sectional data in the *data merging code.do* and call it *author_commits*
    - Unique number of participants that are commiting to the eip github for each eip. We call this variable *contributors* and merge this with the cross-sectional data in the *data merging code.do*
* **Client Commit** are commitments to the *Client Repositories* These commitments may be for anything even things unrelated to EIPs.
* - We begin with python code *???????.py* which creates four stata files *commitsbesu.dta*,*commitserigon.dta*,*commitsgeth.dta*
  - We then aggregate all commits by each github username
  - We match github usernames with authors to see how many commits are done by eip authors
  - We then merge this to the cross-sectional data for each author 1 to 11
  - We create a maximum of all authors for eip. This process is repeated for 4 clients so we get *geth_commits*, *besu_commits*, *erigon_commits*, and *nethermind_commits* as four variables in the cross-sectional data. The process is included in the *data merging code.do*
## Create Betweenness Centrality Measure
we use python code *??????.py* to create a betweenness centrality measure amongs all co-authors of EIPs. The code takes file *Ethereum_Cross-Sectional_Data.csv* and create a betweenness centrality measure for each EIP based on author_ids. We merge the betweenness_centrality measure in the Ethereum_Cross-sectional_Date.dta using eip_number.
## Preparing Data for Regression
Final step is to prepare the cross-sectional data for regression. This is done using the stata code *Preparing Regression Data.do* The preparation entails finding top 10 companies represented by authors in the Cross-sectional data. We count just the company1 where authors work and find the frequency of authors by each company. We then sort it and create dummy variables for top 10 companies. In addition to using company dummies, the preparation code also create a success variable that uses 0 and 1 for eips in progress and finalized. 
